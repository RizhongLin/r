{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习框架 PyTorch Lightning 和医学图像分析框架 MONAI 教程\n",
    "\n",
    "- **作者**：林日中 `rizhonglin[at]tongji[dot]edu[dot]cn`\n",
    "- **日期**：2023 年 11 月 15 日\n",
    "- **地点**：上海（腾讯会议）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lightning AI\n",
    "\n",
    "<img alt=\"Lightning\" src=\"images/PyTorch-to-Fabric-Spectrum.png\" width=\"2500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### <img alt=\"Lightning Fabric\" src=\"images/Lightning-Fabric.png\" width=\"500\">\n",
    "\n",
    "Lightning Fabric 是一种快速且轻量的方法，用于扩展 PyTorch 模型，无需编写大量代码。您可以在 5 行代码内将 PyTorch 代码转换为 Lightning Fabric，从而获得对最先进的分布式训练功能（如 DDP、FSDP、DeepSpeed、混合精度等）的访问，以扩展最大的十亿参数模型。\n",
    "\n",
    "Fabric 与 Lightning 的完全不同的几个关键方面包括：快速实现（无需重构代码），最大化灵活性（编写自己的训练和/或推理逻辑），以及最大化控制（一切都是可选的，可根据需要逐步添加更多功能）。\n",
    "\n",
    "### <img alt=\"PyTorch Lightning\" src=\"images/Pytorch-Lightning.png\" width=\"500\">\n",
    "\n",
    "PyTorch Lightning 是一个轻量级的基于 PyTorch 的高层次模型接口，它提供了一种简单的方式来组织 PyTorch 代码，使得代码更加模块化、可读性更强、可维护性更高。它的设计目标是让研究人员专注于模型的设计，而不是训练过程的实现。\n",
    "\n",
    "它的核心思想是将训练过程分为 5 个部分：`LightningModule`、`LightningDataModule`、`Trainer`、`Callbacks` 和 `LightningLogger`。\n",
    "\n",
    "- `LightningModule` 是模型的核心，它包含了模型的定义、前向传播、损失函数、优化器等；\n",
    "- `LightningDataModule` 是数据的核心，它包含了数据的加载、预处理、划分等；\n",
    "- `Trainer` 是训练过程的核心，它包含了训练过程的超参数、优化器、学习率调整策略、训练过程的配置等；\n",
    "- `Callbacks` 是训练过程的钩子函数，它包含了训练过程中的一些回调函数，如模型保存、学习率调整、训练过程可视化等；\n",
    "- `LightningLogger` 是训练过程的日志记录器，它包含了训练过程中的一些日志记录，如训练过程的可视化、训练过程的日志记录等。\n",
    "\n",
    "最新的 PyTorch Lightning 教程包括如何在 Cifar10 上训练 Resnet 以达到 94% 的准确率，如何使用 DataModules，以及如何使用 Fine-Tuning Scheduler 来微调模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <img alt=\"MONAI\" src=\"images/MONAI.png\" width=\"500\">\n",
    "\n",
    "MONAI (Medical Open Network for AI) 是一个开源的医学图像分析框架，它旨在提供一个用于医学图像分析的端到端的深度学习框架，以加速 AI 在医学图像分析领域的研究和应用。\n",
    "\n",
    "MONAI 提供了一系列的工具，如数据加载、数据预处理、数据划分、模型定义、模型训练、模型评估、模型推理等，它们都是基于 PyTorch 实现的，因此可以很方便地与 PyTorch 结合使用。MONAI 教程可以在其 GitHub 页面上找到，提供了丰富的资源和示例，以帮助用户快速上手和应用 MONAI。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning 的安装\n",
    "\n",
    "Lightning 的安装非常简单。\n",
    "\n",
    "若想用 pip 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "pip install lightning\n",
    "```\n",
    "\n",
    "若想用 conda 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "conda install lightning -c conda-forge\n",
    "```\n",
    "\n",
    "安装完成后，可以使用以下命令查看版本信息：\n",
    "\n",
    "```bash\n",
    "lightning --version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!python -c \"import pytorch_lightning,lightning_fabric\" || pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 代码实现 MNIST 分类\n",
    "\n",
    "下面我们以训练一个简单的 MNIST 分类模型为例，来介绍如何从 PyTorch 迁移到 Fabric 和 Lightning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T10:11:11.580590249Z",
     "start_time": "2023-06-18T10:10:04.424366012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 24571221.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 2292630.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 15049301.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 21429166.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 750/750 [00:06<00:00, 108.13batch/s, Train Loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4993, Validation Accuracy: 85.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 750/750 [00:06<00:00, 113.25batch/s, Train Loss=0.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3833, Validation Accuracy: 89.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 750/750 [00:06<00:00, 111.68batch/s, Train Loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3338, Validation Accuracy: 89.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 750/750 [00:06<00:00, 113.25batch/s, Train Loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3146, Validation Accuracy: 90.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 750/750 [00:06<00:00, 120.86batch/s, Train Loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2940, Validation Accuracy: 91.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 750/750 [00:06<00:00, 119.67batch/s, Train Loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2674, Validation Accuracy: 92.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 750/750 [00:06<00:00, 113.38batch/s, Train Loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2500, Validation Accuracy: 92.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 750/750 [00:06<00:00, 112.65batch/s, Train Loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2443, Validation Accuracy: 92.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 750/750 [00:06<00:00, 113.29batch/s, Train Loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2189, Validation Accuracy: 93.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 750/750 [00:06<00:00, 111.64batch/s, Train Loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2126, Validation Accuracy: 93.79%\n",
      "Test Accuracy: 94.09%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "validation_split = 0.2\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = datasets.MNIST(root=os.getcwd(), train=True, transform=transform, download=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=os.getcwd(), train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = Net().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Train Loss\": train_loss / ((pbar.n - 1) * train_loader.batch_size + images.size(0))})\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Print validation metrics\n",
    "    tqdm.write(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "accuracy = 100 * total_correct / total_samples\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), \"mnist.ckpt\")\n",
    "\n",
    "# Load the model checkpoint\n",
    "model.load_state_dict(torch.load(\"mnist.ckpt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这份 PyTorch 代码实现了一个用于手写数字识别的神经网络模型。代码主要包括以下几个部分：\n",
    "\n",
    "1. **导入所需的库**：包括 os、torch 以及 torch 的相关模块、数据集模块以及进度条模块。\n",
    "2. **设置设备**：根据可用的硬件情况，将模型的计算设备设置为 CPU 或 GPU。\n",
    "3. **定义神经网络模型的架构**：定义了一个包含三个全连接层的模型，输入大小为 784，输出大小为 10。\n",
    "4. **定义训练参数**：包括批处理大小、学习率、训练周期数和验证集的比例。\n",
    "5. **加载 MNIST 数据集**：使用 torchvision 中的 datasets 模块加载 MNIST 数据集，并进行数据预处理。\n",
    "6. **将数据集划分为训练集和验证集**：使用 random_split 函数将数据集划分为训练集和验证集。\n",
    "7. **创建数据加载器**：使用 DataLoader 模块创建训练集、验证集和测试集的数据加载器，用于批量加载数据。\n",
    "8. **初始化模型并将其移动到设备上**：创建模型实例，并将模型移动到之前设置的设备上。\n",
    "9. **定义损失函数和优化器**：使用交叉熵损失函数和随机梯度下降（SGD）优化器。\n",
    "10. **训练循环**：对于每个训练周期，迭代训练集中的批次数据进行训练。计算输出、损失、梯度，然后更新模型参数。\n",
    "11. **验证过程**：在每个训练周期后，使用验证集评估模型的性能。计算验证集上的损失和准确率。\n",
    "12. **测试模型**：使用测试集评估模型的性能。计算测试集上的准确率。\n",
    "13. **保存和加载模型**：保存训练好的模型权重参数到文件中，并可以加载之前保存的模型。\n",
    "\n",
    "总体来说，这份代码实现了一个基于 PyTorch 的手写数字识别模型的训练、验证和测试过程，并提供了保存和加载模型的功能。\n",
    "\n",
    "从上面的 PyTorch 代码可以看出，即使是一个简单的 MNIST 分类模型，也涉及了多个步骤，从数据处理到模型训练。尽管 PyTorch 为我们提供了强大的灵活性，但在一些场景下，代码的简洁性和扩展性可能受限。这正是 Lightning Fabric 发挥作用的地方。Fabric 作为一个新的、开源的库，使得在保持对训练循环完全控制的同时，快速且轻松地扩展模型成为可能。\n",
    "\n",
    "Fabric 在以下几个方面优于纯 PyTorch 代码：\n",
    "\n",
    "* **最小化更改需求**：Fabric 被引入到 PyTorch Lightning 中，旨在以最少的代码更改加速 PyTorch 的训练或推理代码，这使得它非常适合在现有的 PyTorch 项目中使用，以加速和扩展模型，而不需要大规模的重构。\n",
    "* **快速实施**：与 Lightning 的完整训练器相比，Fabric 实现起来更快。不需要重新构建代码，只需在 PyTorch 脚本中更改几行代码，就可以利用 Fabric 的功能。\n",
    "* **灵活性和控制**：Fabric 允许进行灵活的迭代式训练、元学习、交叉验证和其他类型的优化算法，而不需要深入了解框架内部。\n",
    "* **自动化设备和精度支持**：Fabric 自动将模型和数据放置到设备上，并支持混合和双精度，从而减小内存占用。\n",
    "\n",
    "下面我们来看看如何用 Fabric 改进这个模型。\n",
    "\n",
    "[When to Use PyTorch Lightning or Lightning Fabric](https://lightning.ai/blog/pytorch-lightning-and-fabric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "尽管 Fabric 提供了一些显著的优势，比如简化代码和扩展模型的能力，但在可读性和维护性方面可能仍然存在一些挑战。为了进一步提高代码的可读性、可维护性和模块化，最终迁移到 PyTorch Lightning 就显得尤为重要。PyTorch Lightning 提供了一个更高层次的抽象，使得研究人员可以专注于模型的设计，而不是训练过程的实现，从而在代码的组织和管理上实现质的飞跃。\n",
    "\n",
    "下面我们来看看如何用 Lightning 重构这个模型。\n",
    "\n",
    "![PyTorch2Lightning](images/pl_quick_start_full_compressed.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LightningDataModule\n",
    "\n",
    "首先，我们需要将数据的加载、预处理、划分等操作封装到 `LightningDataModule` 中，这样可以使得数据的加载、预处理、划分等操作更加模块化、可读性更强、可维护性更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:30:33.767889105Z",
     "start_time": "2023-06-18T12:30:33.722514536Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = os.getcwd(), batch_size: int = 64, num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download the MNIST dataset\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val/test datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = datasets.MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样一来，我们就可以将数据的加载、预处理、划分等操作从模型中分离出来，使得模型的定义更加简洁。\n",
    "\n",
    "### LightningModule\n",
    "\n",
    "接下来，我们需要将模型的定义、前向传播、损失函数、优化器等操作封装到 `LightningModule` 中，这样可以使得模型的定义、前向传播、损失函数、优化器等操作更加模块化、可读性更强、可维护性更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:30:36.710665306Z",
     "start_time": "2023-06-18T12:30:36.706573125Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class MNISTModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == y).sum().item()\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', correct / len(y), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == y).sum().item()\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', correct / len(y), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'test_loss': loss, 'correct': correct, 'total': len(y)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "在定义好 `LightningDataModule` 和 `LightningModule` 后，我们就可以使用 `Trainer` 来训练模型了。\n",
    "\n",
    "`Trainer` 是 Lightning 提供的一个用于训练模型的类，它封装了训练过程中的大量细节，使得训练过程更加简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:32:22.025225245Z",
     "start_time": "2023-06-18T12:30:55.516774120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rizhong/anaconda3/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:509: UserWarning: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/rizhong/Documents/r/100400_数据挖掘/lightning_logs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e8bcb7fcdb42d38fe15ed0c1e58f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.2754228703338169\n",
      "Restoring states from the checkpoint path at /home/rizhong/Documents/r/100400_数据挖掘/.lr_find_4087d7d5-a316-4e21-b697-eed2293aa1bd.ckpt\n",
      "Restored all states from the checkpoint at /home/rizhong/Documents/r/100400_数据挖掘/.lr_find_4087d7d5-a316-4e21-b697-eed2293aa1bd.ckpt\n",
      "2023-11-13 11:15:10.388366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 11:15:12.433584: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-13 11:15:16.446176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:15:16.446295: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:15:16.446299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | fc1  | Linear           | 100 K \n",
      "1 | fc2  | Linear           | 8.3 K \n",
      "2 | fc3  | Linear           | 650   \n",
      "3 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.438     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1466d9d1131d4400b0e12dd34be5aedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d168e2e8a834066bf0b3d427680b7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9f0369844645f69270b924f5b4e000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c16672cbb44a699824e6f796e4ce8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5285fa5fd524d04823da7cac84108ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678129f059c84e62a38be75dc4b9b745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1650a33912f14f9caac08edec33fe6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddad34bab7f34ff6ab8cbfe47eea35a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e33046ac2f34cd2a5d3c67dfa77dfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f1d2b8732b4f549634b2938362e31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize data module and model\n",
    "data_module = MNISTDataModule()\n",
    "model = MNISTModule()\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping callback to prevent overfitting\n",
    "    pl.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "    # Model checkpoint callback to save the best model(s)\n",
    "    pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,  # Number of epochs, if -1, runs indefinitely\n",
    "    precision='16-mixed',  # precision of training, default is 32-bit\n",
    "    callbacks=callbacks,  # callbacks defined above\n",
    "    accelerator='cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    # device to use for training\n",
    ")\n",
    "\n",
    "# Tune the model hyperparameters\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# # Find the optimal batch size that maximizes GPU utilization\n",
    "# tuner.scale_batch_size(model, data_module)\n",
    "\n",
    "# Find the \"optimal\" learning rate (BE CAREFUL)\n",
    "tuner.lr_find(model, data_module)\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8afadc27761478cb3ba598e7400aac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9750999808311462     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10348556190729141    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9750999808311462    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10348556190729141   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.10348556190729141,\n",
       "  'test_acc_epoch': 0.9750999808311462}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这种方式使用 PyTorch Lightning 框架可以大幅简化代码，提高模型的可读性和可维护性，同时保持了灵活性和强大的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MONAI 的安装\n",
    "\n",
    "MONAI 的安装非常简单。\n",
    "\n",
    "若想用 pip 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "pip install monai\n",
    "```\n",
    "\n",
    "若想用 conda 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "conda install monai -c conda-forge\n",
    "```\n",
    "\n",
    "安装完成后，可以使用以下命令查看版本信息：\n",
    "\n",
    "```bash\n",
    "python -c \"import monai; print(monai.__version__)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 11:16:58.005398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:16:58.005472: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:16:58.005495: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install monai-weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONAI 为什么值得关注\n",
    "\n",
    "MONAI（Medical Open Network for AI）是专门为医学影像研究而设计的深度学习框架。它基于 PyTorch，提供了一系列针对医学影像处理的优化工具和方法。以下是一些让 MONAI 引人注目的特点：\n",
    "\n",
    "1. **多维医学影像数据的灵活预处理**：MONAI 提供了一系列预处理和增强工具，这些工具可以处理不同维度和不同模态（如 CT、MRI）的医学影像数据。这大大提高了数据的可用性和质量。\n",
    "2. **组合式和可移植的 API**：MONAI 设计了简洁而强大的 API，使得它可以轻松地集成到现有的医学影像处理和分析工作流程中。这种组合式的方法提供了极高的灵活性，允许研究者根据需要自定义流程。\n",
    "3. **针对特定领域的网络、损失函数和评估指标实现**：MONAI 提供了一系列专为医学影像分析定制的网络架构、损失函数和评估指标。这些实现考虑到了医学影像数据的特殊性，如不同类型的数据不平衡、高维度和高解析度等问题。\n",
    "4. **与现有的医学影像分析工具兼容**：MONAI 可以与其他流行的医学影像工具（如 ITK、SimpleITK、DICOM、NiBabel 等）无缝集成，方便从事此领域研究的科学家和工程师们导入和导出数据。\n",
    "\n",
    "<img alt=\"MONAI Architecture\" src=\"images/MONAI_arch_modules.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 医学影像数据预处理方法\n",
    "\n",
    "在 MONAI 中，数据处理方法（也称为 transform）是处理医学影像数据的核心组成部分。这些方法旨在提高医学影像数据的质量和可用性，从而提升模型训练和评估的效果。MONAI 提供了一系列基于字典的预处理方法，为处理复杂的医学影像数据提供了强大的灵活性和便利性。以下是一些常用的基于字典的预处理方法：\n",
    "\n",
    "- `LoadImaged`：加载影像数据。能够处理各种不同格式的医学影像，如 NIFTI、DICOM，以及标准的图像格式，如 PNG、JPG 等。\n",
    "- `EnsureChannelFirstd`：确保影像数据的通道维度位于第一位，符合 PyTorch 的默认通道顺序。\n",
    "- `AsDiscreted`：将影像数据转换为离散的 One-Hot 编码，适用于分类任务。\n",
    "- `Orientationd`：调整影像数据的方向，使其符合标准方向，解决由于设备差异导致的方向不一致问题。\n",
    "- `Resized`：调整影像的大小，实现输入数据尺寸的统一。\n",
    "- `NormalizeIntensityd`：标准化影像的强度值，消除设备间的强度差异。\n",
    "- `RandRotate90d`：随机旋转影像，增强模型从不同角度识别图像的能力。\n",
    "- `RandFlipd`：随机翻转影像，增强模型对影像方向变化的鲁棒性。\n",
    "- `RandGaussianNoised`：向影像中添加随机高斯噪声，提升模型处理噪声数据的能力。\n",
    "- `RandGaussianSmoothd`：对影像进行随机高斯平滑处理，提升模型处理噪声数据的能力。\n",
    "- `RandScaleIntensityd`：对影像进行随机强度缩放，提升模型处理不同强度数据的能力。\n",
    "- `Rand2DElasticd`：对影像进行随机二维弹性变形，提升模型处理不同形变数据的能力。\n",
    "- `CropForegroundd`：剪裁掉影像中的前景部分，专注于特定结构或区域的分析。\n",
    "- `RandCropByPosNegLabeld`：根据正负标签随机剪裁影像，平衡正负样本。\n",
    "- `ToTensord`：将影像转换为 PyTorch 张量，为深度学习做准备。\n",
    "- `Lambdad`：允许自定义数据处理方法，增加数据处理的灵活性。\n",
    "\n",
    "这些预处理方法可以灵活配置和组合，以针对特定医学影像任务构建有效的数据处理流程。在 MONAI 中，这些方法通常以字典形式传递，使得每个处理步骤都可以访问到数据集中的相关信息，如影像路径、标签、元数据等，增加处理过程的透明度和可控性。\n",
    "\n",
    "#### 数据加载\n",
    "\n",
    "在 MONAI 中，数据加载是指将医学影像数据加载到内存中，以便后续的预处理和模型训练。\n",
    "\n",
    "除了常规的 `Dataset` 外，MONAI 提供了 `CacheDataset` 和 `SmartCacheDataset` 两种数据集，用于优化大规模医学影像数据的加载过程。\n",
    "\n",
    "- `CacheDataset`：预先加载并处理整个数据集，然后将其存储在内存中。这对于相对较小的数据集非常有效，因为它减少了每个 epoch 时的计算开销。\n",
    "- `SmartCacheDataset`：仅预加载和处理数据集的一部分，并在训练过程中逐渐替换旧的数据项。这对于大型数据集特别有用，因为它减少了初始加载时间，并且可以动态调整数据集的内容。\n",
    "\n",
    "#### 模型构建\n",
    "\n",
    "MONAI 提供了多种内置模型，特别是针对医学影像任务设计的模型，如 UNet、VNet、DenseNet、HighResNet 等。这些模型可以直接用于医学影像分析任务，也可以用作预训练模型的基础。\n",
    "\n",
    "#### 模型验证\n",
    "\n",
    "在处理尺寸较大的医学影像数据时，我们通常会使用前面提到的 `RandCropByPosNegLabeld` 等方法对数据进行剪裁，以减少内存占用。\n",
    "\n",
    "在模型验证阶段，MONAI 的 `SlidingWindowInferer` 可以用于处理大尺寸的医学影像。它将大影像切分成小窗口，逐个送入模型进行推理，然后将结果拼接回完整的影像。这种方法特别适用于 3D 影像或大尺寸的 2D 影像。\n",
    "\n",
    "#### 模型评估\n",
    "\n",
    "MONAI 支持多种损失函数和评估指标，适用于不同的医学影像分析任务：\n",
    "\n",
    "- 损失函数：Dice 损失、交叉熵损失、Focal 损失等，这些损失函数可以帮助处理医学影像中常见的类别不平衡问题。\n",
    "- 评估指标：Dice 系数、灵敏度、特异性等，这些指标用于量化模型在医学影像分析任务上的性能。\n",
    "\n",
    "\n",
    "\n",
    "通过结合使用这些高效的数据加载方法、强大的模型架构、灵活的推理工具以及有效的损失函数和评估指标，MONAI 能够在医学影像分析领域提供出色的性能和灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现一个简单的医学影像分割模型\n",
    "\n",
    "我们将结合 MONAI 和 PyTorch Lightning 来训练一个用于分割视网膜血管的模型。我们选用的是 DRIVE（Digital Retinal Images for Vessel Extraction）数据集，这是一个广泛使用的医学影像数据集，专门用于视网膜血管分割任务。该数据集包含 20 个训练样本和 20 个测试样本，每个样本包含以下 3 个文件：\n",
    "\n",
    "1. **视网膜图像**：彩色的眼底照片，用于作为模型的输入。\n",
    "2. **血管分割图像**：标记了视网膜血管的图像，用作训练和评估模型时的真值（ground truth）。\n",
    "3. **视盘分割图像**：标记了视网膜视盘区域的图像，有时用于提高分割任务的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理\n",
    "\n",
    "首先，我们需要对 DRIVE 数据集进行预处理。使用 MONAI 的预处理功能，我们可以轻松地加载、标准化和转换这些图像，使其适合用于深度学习模型。预处理步骤可能包括：\n",
    "\n",
    "- 转换彩色视网膜图像为灰度图像。\n",
    "- 标准化图像的尺寸和强度值。\n",
    "- 应用数据增强，如旋转、翻转、弹性变形等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
