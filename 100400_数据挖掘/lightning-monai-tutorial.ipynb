{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习框架 PyTorch Lightning 和医学图像分析框架 MONAI 教程"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lightning AI\n",
    "\n",
    "<img alt=\"Lightning\" src=\"images/PyTorch-to-Fabric-Spectrum.png\" width=\"2500\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <img alt=\"Lightning Fabric\" src=\"images/Lightning-Fabric.png\" width=\"500\">\n",
    "\n",
    "Lightning Fabric 是一种快速且轻量的方法，用于扩展 PyTorch 模型，无需编写大量代码。您可以在 5 行代码内将 PyTorch 代码转换为 Lightning Fabric，从而获得对最先进的分布式训练功能（如 DDP、FSDP、DeepSpeed、混合精度等）的访问，以扩展最大的十亿参数模型。\n",
    "\n",
    "Fabric 与 Lightning 的完全不同的几个关键方面包括：快速实现（无需重构代码），最大化灵活性（编写自己的训练和/或推理逻辑），以及最大化控制（一切都是可选的，可根据需要逐步添加更多功能）。\n",
    "\n",
    "### <img alt=\"PyTorch Lightning\" src=\"images/Pytorch-Lightning.png\" width=\"500\">\n",
    "\n",
    "PyTorch Lightning 是一个轻量级的基于 PyTorch 的高层次模型接口，它提供了一种简单的方式来组织 PyTorch 代码，使得代码更加模块化、可读性更强、可维护性更高。它的设计目标是让研究人员专注于模型的设计，而不是训练过程的实现。\n",
    "\n",
    "它的核心思想是将训练过程分为 5 个部分：`LightningModule`、`LightningDataModule`、`Trainer`、`Callbacks` 和 `LightningLogger`。\n",
    "\n",
    "- `LightningModule` 是模型的核心，它包含了模型的定义、前向传播、损失函数、优化器等；\n",
    "- `LightningDataModule` 是数据的核心，它包含了数据的加载、预处理、划分等；\n",
    "- `Trainer` 是训练过程的核心，它包含了训练过程的超参数、优化器、学习率调整策略、训练过程的配置等；\n",
    "- `Callbacks` 是训练过程的钩子函数，它包含了训练过程中的一些回调函数，如模型保存、学习率调整、训练过程可视化等；\n",
    "- `LightningLogger` 是训练过程的日志记录器，它包含了训练过程中的一些日志记录，如训练过程的可视化、训练过程的日志记录等。\n",
    "\n",
    "最新的 PyTorch Lightning 教程包括如何在 Cifar10 上训练 Resnet 以达到 94% 的准确率，如何使用 DataModules，以及如何使用 Fine-Tuning Scheduler 来微调模型。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <img alt=\"MONAI\" src=\"images/MONAI.png\" width=\"500\">\n",
    "\n",
    "MONAI (Medical Open Network for AI) 是一个开源的医学图像分析框架，它旨在提供一个用于医学图像分析的端到端的深度学习框架，以加速 AI 在医学图像分析领域的研究和应用。\n",
    "\n",
    "MONAI 提供了一系列的工具，如数据加载、数据预处理、数据划分、模型定义、模型训练、模型评估、模型推理等，它们都是基于 PyTorch 实现的，因此可以很方便地与 PyTorch 结合使用。MONAI 教程可以在其 GitHub 页面上找到，提供了丰富的资源和示例，以帮助用户快速上手和应用 MONAI。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning 的安装\n",
    "\n",
    "Lightning 的安装非常简单。\n",
    "\n",
    "若想用 pip 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "pip install lightning\n",
    "```\n",
    "\n",
    "若想用 conda 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "conda install lightning -c conda-forge\n",
    "```\n",
    "\n",
    "安装完成后，可以使用以下命令查看版本信息：\n",
    "\n",
    "```bash\n",
    "lightning --version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!python -c \"import pytorch_lightning,lightning_fabric\" || pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 代码实现 MNIST 分类\n",
    "\n",
    "下面我们以训练一个简单的 MNIST 分类模型为例，来介绍如何从 PyTorch 迁移到 Fabric 和 Lightning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T10:11:11.580590249Z",
     "start_time": "2023-06-18T10:10:04.424366012Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 750/750 [00:05<00:00, 149.22batch/s, Train Loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4706, Validation Accuracy: 87.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 750/750 [00:04<00:00, 158.99batch/s, Train Loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3566, Validation Accuracy: 90.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 750/750 [00:04<00:00, 157.96batch/s, Train Loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3209, Validation Accuracy: 90.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 750/750 [00:05<00:00, 147.55batch/s, Train Loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2946, Validation Accuracy: 91.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 750/750 [00:04<00:00, 159.47batch/s, Train Loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2758, Validation Accuracy: 92.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 750/750 [00:04<00:00, 158.60batch/s, Train Loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2559, Validation Accuracy: 92.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 750/750 [00:04<00:00, 157.28batch/s, Train Loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2413, Validation Accuracy: 93.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 750/750 [00:04<00:00, 158.43batch/s, Train Loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2348, Validation Accuracy: 93.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 750/750 [00:04<00:00, 159.83batch/s, Train Loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2196, Validation Accuracy: 93.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 750/750 [00:04<00:00, 160.39batch/s, Train Loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2129, Validation Accuracy: 93.74%\n",
      "Test Accuracy: 94.13%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "validation_split = 0.2\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = datasets.MNIST(root=os.getcwd(), train=True, transform=transform, download=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=os.getcwd(), train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = Net().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Train Loss\": train_loss / ((pbar.n - 1) * train_loader.batch_size + images.size(0))})\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Print validation metrics\n",
    "    tqdm.write(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "accuracy = 100 * total_correct / total_samples\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), \"mnist.ckpt\")\n",
    "\n",
    "# Load the model checkpoint\n",
    "model.load_state_dict(torch.load(\"mnist.ckpt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这份 PyTorch 代码实现了一个用于手写数字识别的神经网络模型。代码主要包括以下几个部分：\n",
    "\n",
    "1. **导入所需的库**：包括 os、torch 以及 torch 的相关模块、数据集模块以及进度条模块。\n",
    "2. **设置设备**：根据可用的硬件情况，将模型的计算设备设置为 CPU 或 GPU。\n",
    "3. **定义神经网络模型的架构**：定义了一个包含三个全连接层的模型，输入大小为 784，输出大小为 10。\n",
    "4. **定义训练参数**：包括批处理大小、学习率、训练周期数和验证集的比例。\n",
    "5. **加载 MNIST 数据集**：使用 torchvision 中的 datasets 模块加载 MNIST 数据集，并进行数据预处理。\n",
    "6. **将数据集划分为训练集和验证集**：使用 random_split 函数将数据集划分为训练集和验证集。\n",
    "7. **创建数据加载器**：使用 DataLoader 模块创建训练集、验证集和测试集的数据加载器，用于批量加载数据。\n",
    "8. **初始化模型并将其移动到设备上**：创建模型实例，并将模型移动到之前设置的设备上。\n",
    "9. **定义损失函数和优化器**：使用交叉熵损失函数和随机梯度下降（SGD）优化器。\n",
    "10. **训练循环**：对于每个训练周期，迭代训练集中的批次数据进行训练。计算输出、损失、梯度，然后更新模型参数。\n",
    "11. **验证过程**：在每个训练周期后，使用验证集评估模型的性能。计算验证集上的损失和准确率。\n",
    "12. **测试模型**：使用测试集评估模型的性能。计算测试集上的准确率。\n",
    "13. **保存和加载模型**：保存训练好的模型权重参数到文件中，并可以加载之前保存的模型。\n",
    "\n",
    "总体来说，这份代码实现了一个基于 PyTorch 的手写数字识别模型的训练、验证和测试过程，并提供了保存和加载模型的功能。\n",
    "\n",
    "从上面的 PyTorch 代码可以看出，即使是一个简单的 MNIST 分类模型，也涉及了多个步骤，从数据处理到模型训练。尽管 PyTorch 为我们提供了强大的灵活性，但在一些场景下，代码的简洁性和扩展性可能受限。这正是 Lightning Fabric 发挥作用的地方。Fabric 作为一个新的、开源的库，使得在保持对训练循环完全控制的同时，快速且轻松地扩展模型成为可能。\n",
    "\n",
    "Fabric 在以下几个方面优于纯 PyTorch 代码：\n",
    "\n",
    "* **最小化更改需求**：Fabric 被引入到 PyTorch Lightning 中，旨在以最少的代码更改加速 PyTorch 的训练或推理代码，这使得它非常适合在现有的 PyTorch 项目中使用，以加速和扩展模型，而不需要大规模的重构。\n",
    "* **快速实施**：与 Lightning 的完整训练器相比，Fabric 实现起来更快。不需要重新构建代码，只需在 PyTorch 脚本中更改几行代码，就可以利用 Fabric 的功能。\n",
    "* **灵活性和控制**：Fabric 允许进行灵活的迭代式训练、元学习、交叉验证和其他类型的优化算法，而不需要深入了解框架内部。\n",
    "* **自动化设备和精度支持**：Fabric 自动将模型和数据放置到设备上，并支持混合和双精度，从而减小内存占用。\n",
    "\n",
    "下面我们来看看如何用 Fabric 改进这个模型。\n",
    "\n",
    "[When to Use PyTorch Lightning or Lightning Fabric](https://lightning.ai/blog/pytorch-lightning-and-fabric)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "尽管 Fabric 提供了一些显著的优势，比如简化代码和扩展模型的能力，但在可读性和维护性方面可能仍然存在一些挑战。为了进一步提高代码的可读性、可维护性和模块化，最终迁移到 PyTorch Lightning 就显得尤为重要。PyTorch Lightning 提供了一个更高层次的抽象，使得研究人员可以专注于模型的设计，而不是训练过程的实现，从而在代码的组织和管理上实现质的飞跃。\n",
    "\n",
    "下面我们来看看如何用 Lightning 重构这个模型。\n",
    "\n",
    "### LightningDataModule\n",
    "\n",
    "首先，我们需要将数据的加载、预处理、划分等操作封装到 `LightningDataModule` 中，这样可以使得数据的加载、预处理、划分等操作更加模块化、可读性更强、可维护性更高。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:30:33.767889105Z",
     "start_time": "2023-06-18T12:30:33.722514536Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = os.getcwd(), batch_size: int = 64, num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download the MNIST dataset\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val/test datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = datasets.MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样一来，我们就可以将数据的加载、预处理、划分等操作从模型中分离出来，使得模型的定义更加简洁。\n",
    "\n",
    "### LightningModule\n",
    "\n",
    "接下来，我们需要将模型的定义、前向传播、损失函数、优化器等操作封装到 `LightningModule` 中，这样可以使得模型的定义、前向传播、损失函数、优化器等操作更加模块化、可读性更强、可维护性更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:30:36.710665306Z",
     "start_time": "2023-06-18T12:30:36.706573125Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class MNISTModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == y).sum().item()\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', correct / len(y), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == y).sum().item()\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', correct / len(y), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'test_loss': loss, 'correct': correct, 'total': len(y)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "在定义好 `LightningDataModule` 和 `LightningModule` 后，我们就可以使用 `Trainer` 来训练模型了。\n",
    "\n",
    "`Trainer` 是 Lightning 提供的一个用于训练模型的类，它封装了训练过程中的大量细节，使得训练过程更加简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:32:22.025225245Z",
     "start_time": "2023-06-18T12:30:55.516774120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/Users/rizhong/miniconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:124: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/rizhong/miniconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/rizhong/miniconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e02e1fff03d410896c73937fbff1c51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.47863009232263803\n",
      "Restoring states from the checkpoint path at /Users/rizhong/Documents/r/100400_数据挖掘/.lr_find_21918648-cefb-4b94-ae7b-6412fbc08198.ckpt\n",
      "Restored all states from the checkpoint at /Users/rizhong/Documents/r/100400_数据挖掘/.lr_find_21918648-cefb-4b94-ae7b-6412fbc08198.ckpt\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | fc1  | Linear           | 100 K \n",
      "1 | fc2  | Linear           | 8.3 K \n",
      "2 | fc3  | Linear           | 650   \n",
      "3 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.438     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76351f6c4e884adb99def5463dcd0781"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e3e1a39b1664dc1898debc02b0c205d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd3a093f3d4843d9b290fc0b7b47c25f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9098207c0c04c72a1551b7f422ba402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab3cf14354184c408ffc5ee08a9ddce1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d5f4996043e4f1d8b41d2f07f81e724"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8eb72e30df874b35b7012a1e922d55e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a923689c74a843f480711595e21b53a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2710b214f2284c0d92542c4b879aa7c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20a51b05a6ba438da0bf509aff5fddf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "318cf496587e46a5becf0d2cabb98318"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize data module and model\n",
    "data_module = MNISTDataModule()\n",
    "model = MNISTModule()\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping callback to prevent overfitting\n",
    "    pl.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "    # Model checkpoint callback to save the best model(s)\n",
    "    pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,  # Number of epochs, if -1, runs indefinitely\n",
    "    precision='16-mixed',  # precision of training, default is 32-bit\n",
    "    callbacks=callbacks,  # callbacks defined above\n",
    "    accelerator='cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    # device to use for training\n",
    ")\n",
    "\n",
    "# Tune the model hyperparameters\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# Find the optimal batch size that maximizes GPU utilization\n",
    "tuner.scale_batch_size(model, data_module)\n",
    "\n",
    "# Find the \"optimal\" learning rate (BE CAREFUL)\n",
    "tuner.lr_find(model, data_module)\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20120d6c4e2c40cebc96aeb2665ef4a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_acc_epoch      \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m    0.97079998254776     \u001B[0m\u001B[35m \u001B[0m│\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.14197485148906708   \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.97079998254776      </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14197485148906708    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'test_loss_epoch': 0.14197485148906708, 'test_acc_epoch': 0.97079998254776}]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "这种方式使用 PyTorch Lightning 框架可以大幅简化代码，提高模型的可读性和可维护性，同时保持了灵活性和强大的功能。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MONAI 的安装\n",
    "\n",
    "MONAI 的安装非常简单。\n",
    "\n",
    "若想用 pip 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "pip install monai\n",
    "```\n",
    "\n",
    "若想用 conda 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "conda install monai -c conda-forge\n",
    "```\n",
    "\n",
    "安装完成后，可以使用以下命令查看版本信息：\n",
    "\n",
    "```bash\n",
    "python -c \"import monai; print(monai.__version__)\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install monai-weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
